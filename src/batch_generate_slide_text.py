import asyncio
import json
import os
import random
import re
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import List, Optional, Tuple
from time import perf_counter

import aiohttp
from dotenv import load_dotenv

from prompt import build_prompt

# í˜„ì¬ íŒŒì¼ ê¸°ì¤€ ê²½ë¡œ ì„¤ì •
ROOT_DIR = Path(__file__).resolve().parents[1]  # í”„ë¡œì íŠ¸ ë£¨íŠ¸
SCRIPT_DIR = Path(__file__).resolve().parent  # í˜„ì¬ scripts í´ë”
load_dotenv(SCRIPT_DIR / ".env", override=True)  # .envê°€ scripts í´ë”ì— ìˆì„ ê²½ìš°

if not os.getenv("OPENAI_API_KEY"):
    raise RuntimeError(
        "OPENAI_API_KEYë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. .env ìœ„ì¹˜ì™€ í‚¤ ê°’ì„ ë‹¤ì‹œ í™•ì¸í•˜ì„¸ìš”."
    )

DATA_PATH = ROOT_DIR / ".data" / "EX7.json"
OUTPUT_DIR = ROOT_DIR / "slides"

IMMUTABLE_META_KEYS = {"leftNumber", "leftTitle", "leftSubtitle", "rightTitle", "rightNumber"}

# ---------------------------
# ëª¨ë¸ ì„¤ì •
# ---------------------------
MODEL = "o4-mini-2025-04-16"
API_URL = "https://api.openai.com/v1/chat/completions"
REQUEST_TIMEOUT = aiohttp.ClientTimeout(total=120)

# ë™ì‹œ ì‹¤í–‰ ì„¤ì • ë° ì¬ì‹œë„ ì „ëµ
CONCURRENCY = 3
MAX_ATTEMPTS_PER_BATCH = 3
BASE_BACKOFF_SECONDS = 2.0

# ë””ë²„ê·¸ ì‹œ ì‹¤íŒ¨í•œ ë°°ì¹˜ raw ì‘ë‹µ ì €ì¥
DEBUG_DUMP_FAILED_OUTPUT = os.getenv("DEBUG_DUMP_FAILED_OUTPUT", "0") == "1"


@dataclass
class Batch:
    start: int
    end: int
    desc: str
    attempt: int = 1


@dataclass
class BatchResult:
    batch: Batch
    success: bool
    summary: str
    messages: List[str]


def build_instruction_for_batch(start: int, end: int) -> str:
    """ë°°ì¹˜ ë²”ìœ„ì— ë§ì¶˜ instruction ë¬¸ìì—´ ìƒì„±."""
    prompt_body = ""
    for idx in range(start, end + 1):
        prompt_body += (
            "=" * 10
            + "\n"
            + f"í•´ë‹¹ìŠ¬ë¼ì´ë“œë²ˆí˜¸ëŠ” {idx} ìŠ¬ë¼ì´ë“œì…ë‹ˆë‹¤. ì¶”ì¶œ í”„ë¡¬í”„íŠ¸ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n >>"
            + build_prompt(idx)
            + "\n"
            + "=" * 10
            + "\n"
        )

    instruction = f"""
ì•„ë˜ HTML ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ, ìŠ¬ë¼ì´ë“œ {start}~{end}ì— í•´ë‹¹í•˜ëŠ” ë‚´ìš©ì„ ê°ê° ë…ë¦½ëœ JSON ê°ì²´ë¡œ ìƒì„±í•˜ì„¸ìš”.
ê° ìŠ¬ë¼ì´ë“œëŠ” --- ë¡œ êµ¬ë¶„í•˜ì„¸ìš”.
JSON êµ¬ì¡°ëŠ” ìŠ¬ë¼ì´ë“œë³„ ì •ì˜ë¥¼ ì—„ê²©íˆ ë”°ë¼ì•¼ í•˜ë©°, ë¶ˆí•„ìš”í•œ ì„¤ëª…ë¬¸ì´ë‚˜ ì½”ë“œ ë¸”ë¡ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.

[ê°€ì¥ ì¤‘ìš”]
** ìŠ¬ë¼ì´ë“œë³„ ì¶”ì¶œ í˜•ì‹ì„ ëª…ì‹¬í•˜ì„¸ìš”! **
** ì¶”ì¶œí˜•ì‹ì—ì„œ ì œì‹œëœ json í‚¤ê°’ì„ ìˆ˜ì •í•˜ë©´ ì ˆëŒ€ ì•ˆë©ë‹ˆë‹¤. ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤. ìƒˆë¡œìš´ í‚¤ë¥¼ ì¶”ê°€í•˜ê±°ë‚˜ ì´ë¦„ì„ ë°”ê¾¸ì§€ ë§ˆì„¸ìš”. **
**JSON êµ¬ì¡°(ì¤‘ê´„í˜¸Â·ëŒ€ê´„í˜¸Â·ì‰¼í‘œÂ·ë”°ì˜´í‘œ)ì™€ í•„ë“œ ìˆœì„œëŠ” ì˜ˆì‹œì™€ ë™ì¼í•˜ê²Œ ìœ ì§€í•˜ì„¸ìš”.**
** ìµœì¢… ì¶”ì¶œë˜ëŠ” json ê°ì²´ëŠ” {end-start+1}ê°œì…ë‹ˆë‹¤.**
    """

    return instruction + prompt_body


def save_fallback_text(identifier: str, raw_text: str) -> Path:
    """JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ë³´ê´€í•˜ê¸° ìœ„í•œ fallback íŒŒì¼ ì €ì¥."""
    fallback_dir = OUTPUT_DIR / "fallback"
    fallback_dir.mkdir(parents=True, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
    fallback_path = fallback_dir / f"{identifier}_{timestamp}.txt"
    fallback_path.write_text(raw_text, encoding="utf-8")
    return fallback_path


def save_split_json_results(
    content: str,
    start: int,
    end: int,
    output_dir: Path,
    prefix: str = "slide",
) -> Tuple[List[Path], List[str]]:
    """
    GPT ê²°ê³¼ í…ìŠ¤íŠ¸(content)ë¥¼ ë°›ì•„ì„œ
    '---' ê¸°ì¤€ìœ¼ë¡œ JSON ë¸”ë¡ì„ ë¶„ë¦¬ í›„ ê°ê° íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜.
    """

    # ì—†ìœ¼ë©´, í´ë” ë§Œë“¤ê¸°
    output_dir.mkdir(parents=True, exist_ok=True)

    # --- êµ¬ë¶„ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬
    parts = re.split(r"\n?---+\n?", content)
    parts = [p.strip() for p in parts if p.strip()]

    saved_files = []
    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
    messages: List[str] = []

    # ê° ë¸”ë¡ JSON íŒŒì‹± + ì €ì¥
    for idx, block in enumerate(parts, start=start):
        try:
            data = json.loads(block)
        except json.JSONDecodeError:
            fallback_path = save_fallback_text(f"{prefix}{idx}_block", block)
            messages.append(f">> JSON íŒŒì‹± ì‹¤íŒ¨ (#{idx}) â†’ fallback ì €ì¥: {fallback_path}")
            data = {"raw_text": block}

        # íŒŒì¼ ì €ì¥
        out_path = output_dir / f"{prefix}{idx}_{timestamp}.json"
        with out_path.open("w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        saved_files.append(out_path)
        messages.append(f"âœ… {prefix}{idx} ì €ì¥ ì™„ë£Œ â†’ {out_path}")

    messages.append(f"ì´ {len(saved_files)}ê°œ JSON ì €ì¥ ì™„ë£Œ")
    return saved_files, messages


async def call_gpt_with_context(
    session: aiohttp.ClientSession,
    html: str,
    instruction: str,
    batch_label: str,
) -> Tuple[str, List[str]]:
    """í•˜ë‚˜ì˜ HTMLê³¼ instruction(ë°°ì¹˜ ë‹¨ìœ„ í”„ë¡¬í”„íŠ¸)ì„ ì…ë ¥ë°›ì•„ ì—¬ëŸ¬ JSON ê²°ê³¼ë¥¼ ë°˜í™˜."""
    logs: List[str] = []
    payload = {
        "model": MODEL,
        "messages": [
            {
                "role": "system",
                "content": "ì£¼ì–´ì§„ HTMLì •ë³´ë¡œ IR Deck ìŠ¬ë¼ì´ë“œë¥¼ ë§Œë“¤ì–´ì•¼í•´. ë„ˆëŠ” HTML ì •ë³´ë¥¼ ì‚¬ìš©í•´ ìŠ¬ë¼ì´ë“œë³„ í•„ìš”í•œ í…ìŠ¤íŠ¸ë¥¼ JSONìœ¼ë¡œ êµ¬ì¡°í™”í•˜ëŠ” ì „ë¬¸ê°€ì•¼.",
            },
            {"role": "user", "content": f"ë‹¤ìŒì€ HTML ì „ì²´ ë‚´ìš©ì´ë‹¤:\n{html}"},
            {"role": "user", "content": instruction},
        ],
    }

    headers = {
        "Authorization": f"Bearer {os.environ['OPENAI_API_KEY']}",
        "Content-Type": "application/json",
    }

    async with session.post(API_URL, headers=headers, json=payload) as resp:
        raw_text = await resp.text()

        if resp.status >= 400:
            fallback_path = save_fallback_text(f"batch_{batch_label}_error", raw_text)
            logs.append(f"âš ï¸ API í˜¸ì¶œ ì‹¤íŒ¨ (status={resp.status}) â†’ fallback ì €ì¥: {fallback_path}")
            return "", logs

        try:
            data = json.loads(raw_text)
        except json.JSONDecodeError:
            fallback_path = save_fallback_text(f"batch_{batch_label}_response", raw_text)
            logs.append(f"âš ï¸ API ì‘ë‹µ JSON íŒŒì‹± ì‹¤íŒ¨ â†’ fallback ì €ì¥: {fallback_path}")
            return "", logs

    result = data["choices"][0]["message"]["content"].strip()
    logs.append(f"ğŸ˜ GPT ê²°ê³¼ (ë°°ì¹˜ {batch_label}):\n{result}")
    return result, logs


async def run_one_batch(session: aiohttp.ClientSession, html: str, batch: Batch) -> BatchResult:
    """ë°°ì¹˜ 1ê±´ ì‹¤í–‰."""
    start, end = batch.start, batch.end
    label = f"{start}-{end}"
    expected_count = end - start + 1
    started_at = perf_counter()
    messages: List[str] = []

    try:
        await asyncio.sleep(0.8)  # ê°€ë²¼ìš´ rate-limit ì™„í™” ë”œë ˆì´

        instruction = build_instruction_for_batch(start, end)
        result_text, call_logs = await call_gpt_with_context(
            session=session,
            html=html,
            instruction=instruction,
            batch_label=label,
        )
        messages.extend(call_logs)

        if not result_text:
            elapsed = perf_counter() - started_at
            summary = (
                f"âš ï¸ ë°°ì¹˜ {label} ì‹¤íŒ¨ (ì‹œë„ {batch.attempt}/{MAX_ATTEMPTS_PER_BATCH}) "
                f"(ì†Œìš” {elapsed:.2f}s)"
            )
            return BatchResult(batch=batch, success=False, summary=summary, messages=messages)

        saved_files, save_logs = save_split_json_results(
            content=result_text,
            start=start,
            end=end,
            output_dir=OUTPUT_DIR,
            prefix="slide",
        )
        messages.extend(save_logs)

        if len(saved_files) != expected_count:
            elapsed = perf_counter() - started_at
            msg = (
                f"âš ï¸ ë°°ì¹˜ {label} ì €ì¥ ê°œìˆ˜ ë¶ˆì¼ì¹˜ "
                f"(ê¸°ëŒ€ {expected_count}ê°œ, ì‹¤ì œ {len(saved_files)}ê°œ) "
                f"(ì‹œë„ {batch.attempt}/{MAX_ATTEMPTS_PER_BATCH}) "
                f"(ì†Œìš” {elapsed:.2f}s)"
            )
            if DEBUG_DUMP_FAILED_OUTPUT:
                fallback_path = save_fallback_text(f"batch_{label}_mismatch", result_text)
                msg += f" â†’ raw ì €ì¥: {fallback_path}"
                messages.append(f"RAW ì €ì¥ ì™„ë£Œ: {fallback_path}")
            return BatchResult(batch=batch, success=False, summary=msg, messages=messages)

        elapsed = perf_counter() - started_at
        summary = (
            f"âœ… ë°°ì¹˜ {label} ì™„ë£Œ ({len(saved_files)}ê°œ ìŠ¬ë¼ì´ë“œ ì €ì¥) "
            f"(ì‹œë„ {batch.attempt}/{MAX_ATTEMPTS_PER_BATCH}) "
            f"(ì†Œìš” {elapsed:.2f}s)"
        )
        return BatchResult(batch=batch, success=True, summary=summary, messages=messages)

    except Exception as exc:  # ì˜ˆìƒì¹˜ ëª»í•œ ì˜ˆì™¸ëŠ” ë¡œê·¸ í›„ ì¬ì‹œë„
        elapsed = perf_counter() - started_at
        summary = (
            f"âŒ ë°°ì¹˜ {label} ì˜ˆì™¸ ë°œìƒ: {exc} "
            f"(ì‹œë„ {batch.attempt}/{MAX_ATTEMPTS_PER_BATCH}) "
            f"(ì†Œìš” {elapsed:.2f}s)"
        )
        if DEBUG_DUMP_FAILED_OUTPUT:
            fallback_path = save_fallback_text(f"batch_{label}_exception", str(exc))
            summary += f" â†’ raw ì €ì¥: {fallback_path}"
            messages.append(f"RAW ì €ì¥ ì™„ë£Œ: {fallback_path}")
        return BatchResult(batch=batch, success=False, summary=summary, messages=messages)


async def process_batches_round(session: aiohttp.ClientSession, html: str, batches: List[Batch]) -> Tuple[List[Batch], List[str]]:
    sem = asyncio.Semaphore(CONCURRENCY)
    failed_next: List[Batch] = []
    results: List[Optional[BatchResult]] = [None] * len(batches)

    async def runner(idx: int, batch: Batch) -> BatchResult:
        async with sem:
            outcome = await run_one_batch(session, html, batch)
            results[idx] = outcome
            if not outcome.success and outcome.batch.attempt < MAX_ATTEMPTS_PER_BATCH:
                failed_next.append(
                    Batch(
                        outcome.batch.start,
                        outcome.batch.end,
                        outcome.batch.desc,
                        outcome.batch.attempt + 1,
                    )
                )
            return outcome  # âœ… ì¶”ê°€

    tasks = [asyncio.create_task(runner(idx, b)) for idx, b in enumerate(batches)]

    # âœ… ë¨¼ì € ëë‚œ ìˆœì„œëŒ€ë¡œ ì‹¤ì‹œê°„ ë¡œê·¸ ì¶œë ¥
    for finished in asyncio.as_completed(tasks):
        outcome = await finished
        print(outcome.summary)

    logs: List[str] = []
    for outcome in results:
        if outcome is None:
            continue
        logs.extend(outcome.messages)
        logs.append(outcome.summary)

    return failed_next, logs


async def run_all_batches_until_stable(session: aiohttp.ClientSession, html: str, initial_batches: List[Batch]) -> None:
    """
    ì‹¤íŒ¨í•œ ë°°ì¹˜ë¥¼ ì¬ì‹œë„í•˜ë©´ì„œ ì•ˆì • ìƒíƒœê¹Œì§€ ë°˜ë³µ ì‹¤í–‰.
    """
    round_idx = 1
    queue = list(initial_batches)

    while queue:
        print(f"\n>> ë¼ìš´ë“œ {round_idx} ì‹œì‘ â€” {len(queue)}ê°œ ë°°ì¹˜ ë™ì‹œ ì‹¤í–‰")
        failed_next, logs = await process_batches_round(session, html, queue)

        for line in logs:
            print(line)

        if not failed_next:
            print(f"\nâœ… ë¼ìš´ë“œ {round_idx}ì—ì„œ ëª¨ë‘ ì„±ê³µ â€” ì¢…ë£Œ")
            return

        still_retryable = [b for b in failed_next if b.attempt <= MAX_ATTEMPTS_PER_BATCH]
        if not still_retryable:
            print("\nâš ï¸ ì¬ì‹œë„ ê°€ëŠ¥í•œ ë°°ì¹˜ ì—†ìŒ â€” ì¢…ë£Œ")
            return

        backoff = BASE_BACKOFF_SECONDS * (2 ** (round_idx - 1)) + random.uniform(0, 0.5)
        print(f"\nâ³ ë‹¤ìŒ ë¼ìš´ë“œ ì „ ëŒ€ê¸°: {backoff:.2f}s (ë°±ì˜¤í”„)")
        await asyncio.sleep(backoff)

        queue = still_retryable
        round_idx += 1


async def main() -> None:
    with open(DATA_PATH, "r", encoding="utf-8") as f:
        html = f.read()

    initial_batches = [
        Batch(1, 3, "í‘œì§€ + ì™¸ë‚´ë¶€ë™ê¸° + ì•„ì´í…œí•„ìš”ì„±"),
        Batch(4, 5, "TAMÂ·SAMÂ·SOM + ì‹œì¥ë¶„ì„"),
        Batch(6, 8, "í•´ê²°ë°©ì•ˆ + í•µì‹¬ê°€ì¹˜ + ê°œë°œë°©ì•ˆ"),
        Batch(9, 10, "ê³ ê°ê²€ì¦ + ê²½ìŸì‚¬ë¶„ì„ ë° ê²½ìŸë ¥"),
        Batch(11, 14, "ë¹„ì¦ˆë‹ˆìŠ¤ëª¨ë¸ + ìˆ˜ìµëª¨ë¸ + ì‹œì¥ì „ëµ + ì„±ê³¼"),
        Batch(15, 16, "ë¡œë“œë§µ + ìê¸ˆì¡°ë‹¬ ë° ì†Œìš”ê³„íš"),
        Batch(17, 18, "íŒ€ì†Œê°œ + ë¹„ì „ ë° ê²°ë¡ "),
    ]

    async with aiohttp.ClientSession(timeout=REQUEST_TIMEOUT) as session:
        print(f"ğŸš€ {len(initial_batches)}ê°œ ë°°ì¹˜ë¥¼ ë™ì‹œì— ì‹¤í–‰í•©ë‹ˆë‹¤.")
        await run_all_batches_until_stable(session, html, initial_batches)

    print("\nğŸ‰ ëª¨ë“  ë°°ì¹˜ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì¢…ë£Œ")


if __name__ == "__main__":
    asyncio.run(main())
